# -*- coding: utf-8 -*-
"""cache-implementation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hyRU68SB4BNOcvnlMACaanQgAOfxcuzr
"""

!pip install -U sentence-transformers

from sentence_transformers import SentenceTransformer, util
import joblib
import numpy as np
import pandas as pd

embedder = SentenceTransformer('all-MiniLM-L6-v2')
model = joblib.load('questions-categorizer-v2-KNeighborsClassifier.model.joblib')

# cache files
cache_for_VM = pd.read_csv('responses-for-virtual-memory.csv').values

# encoded cache files
embeddings_cache_for_VM = embedder.encode(cache_for_VM[:, 1])

def get_category(input_request):
  encoded_input_request = embedder.encode(input_request)
  encoded_input_request = encoded_input_request /  np.linalg.norm([encoded_input_request], axis=1, keepdims=True)
  selected_category = model.predict(encoded_input_request)
  return selected_category

def give_the_response(new_request, category):
  encoded_new_request = embedder.encode(new_request)

  response = cache_handler(new_request, encoded_new_request, category)
  return response

def cache_handler(request, encoded_request, category):
  if category == 0:
    cos_sim = util.cos_sim(embeddings_cache_for_VM, encoded_request)
    if (max(cos_sim) > 0.75):
      print("* * * * * * * From Cache * * * * * * * ")
      return cache_for_VM[cos_sim.argmax()][2]
    else:
      return call_API(request)
  # add other categories

def call_API(request):
  print("* * * * * * * Calling API * * * * * * * ")
  mock_resp = "API response for -> " + request
  return mock_resp


test_sentence = "can you please tell me about virtual memory?"

category = get_category(test_sentence)
response_for_test_sentence = give_the_response(test_sentence, category)
print(response_for_test_sentence)

